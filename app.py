import os
import openai
import streamlit as st
import base64
from difflib import SequenceMatcher


from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import numpy as np
import threading
import queue


openai.api_key = st.secrets['API_KEY']


initial_script = [
    "Narrator: It's a beautiful fall morning on the farm.",
    "Narrator: The leaves are turning yellow and red.",
    "Narrator: Fern comes to visit Wilbur, her favorite pig.",
    "Fern: Good morning, Wilbur! How are you today?",
    "Wilbur: Oh, Fern! I'm so happy to see you. I was feeling a little lonely.",
    "Fern: Don't be lonely, Wilbur. You have so many friends here on the farm!",
    "Charlotte: Good morning, Fern. You're right, Wilbur has many friends, including me.",
    "Wilbur: Charlotte! I'm so glad you're here. Fern, isn't Charlotte amazing? She can make the most beautiful webs."
]

if 'current_line' not in st.session_state:
    st.session_state.current_line = 0
if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []

st.markdown("""
<style>
    .main {
        background-color: #f0f8ff;
        padding: 20px;
        border-radius: 10px;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border: none;
        border-radius: 5px;
        padding: 10px 20px;
    }
    .stTextInput>div>div>input {
        background-color: #e6f3ff;
        border-radius: 5px;
    }
    h1 {
        color: #2E8B57;
        text-align: center;
    }
    h2 {
        color: #4682B4;
    }
    .script-line {
        background-color: white;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 5px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ•·ï¸ ìƒ¬ë¡¯ì˜ ê±°ë¯¸ì¤„ ì¸í„°ë™í‹°ë¸Œ í•™ìŠµ ğŸ·")

def text_to_speech(text):
    
    from gtts import gTTS
    from io import BytesIO

    tts = gTTS(text=text, lang='en')
    audio_bytes = BytesIO()
    tts.write_to_fp(audio_bytes)
    audio_bytes.seek(0)

    st.audio(audio_bytes.read(), format='audio/mp3')

def generate_response(prompt):
    st.session_state.conversation_history.append({"role": "user", "content": prompt})

    try:
        chat_completion = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an AI tutor helping a student learn English through the story of Charlotte's Web. Provide explanations, answer questions, and engage in dialogue about the story, characters, and language used. Keep your responses appropriate for young learners."},
                *st.session_state.conversation_history
            ]
        )

        ai_response = chat_completion.choices[0].message.content.strip()
        st.session_state.conversation_history.append({"role": "assistant", "content": ai_response})
        return ai_response
    except Exception as e:
        st.error(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

def recognize_speech():
    
    pass  

def evaluate_speech_accuracy(original_text, recognized_text):
    similarity = SequenceMatcher(None, original_text.lower(), recognized_text.lower()).ratio()
    return similarity * 100

st.sidebar.header("ì „ì²´ ëŒ€ë³¸")
for i, line in enumerate(initial_script):
    st.sidebar.markdown(f'<div class="script-line">{line}</div>', unsafe_allow_html=True)
    if st.sidebar.button(f"ğŸ”Š ë“£ê¸°", key=f"listen_{i}"):
        text_to_speech(line)


st.header("ğŸ§ ìˆœì°¨ì  ë“£ê¸°")
col1, col2, col3 = st.columns(3)
with col1:
    if st.button("â®ï¸ ì´ì „ ì¤„") and st.session_state.current_line > 0:
        st.session_state.current_line -= 1

with col2:
    if st.button("â–¶ï¸ í˜„ì¬ ì¤„ ë“£ê¸°"):
        text_to_speech(initial_script[st.session_state.current_line])

with col3:
    if st.button("â­ï¸ ë‹¤ìŒ ì¤„") and st.session_state.current_line < len(initial_script) - 1:
        st.session_state.current_line += 1

st.info(f"í˜„ì¬ ì¤„: {initial_script[st.session_state.current_line]}")


st.header("ğŸ’¬ ì¸í„°ë™í‹°ë¸Œ í•™ìŠµ")
input_method = st.radio("ì…ë ¥ ë°©ë²• ì„ íƒ:", ("í…ìŠ¤íŠ¸", "ìŒì„±"))

if input_method == "í…ìŠ¤íŠ¸":
    user_input = st.text_input("ìŠ¤í† ë¦¬, ë“±ì¥ì¸ë¬¼ ë˜ëŠ” ì–¸ì–´ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”:")
else:
    st.write("ë…¹ìŒì„ ì‹œì‘í•˜ë ¤ë©´ 'Start'ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
    
    RTC_CONFIGURATION = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})

    from streamlit_webrtc import AudioProcessorBase

    class AudioProcessor(AudioProcessorBase):
        def __init__(self):
            self.audio_frames = []

        def recv(self, frame):
            # ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì§‘
            self.audio_frames.append(frame)
            return frame

    webrtc_ctx = webrtc_streamer(
        key="speech-recognition",
        mode=WebRtcMode.SENDONLY,
        rtc_configuration=RTC_CONFIGURATION,
        media_stream_constraints={"audio": True, "video": False},
        audio_processor_factory=AudioProcessor,
        async_processing=True,
    )

    if webrtc_ctx.audio_processor:
        if st.button("ì˜¤ë””ì˜¤ ì²˜ë¦¬í•˜ê¸°"):
            st.write("ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
           
            audio_frames = webrtc_ctx.audio_processor.audio_frames

          
            from pydub import AudioSegment
            from io import BytesIO

            combined = AudioSegment.empty()
            for frame in audio_frames:
                audio = frame.to_ndarray()
                sound = AudioSegment(
                    data=audio.tobytes(),
                    sample_width=audio.dtype.itemsize,
                    frame_rate=frame.sample_rate,
                    channels=len(frame.layout.channels),
                )
                combined += sound

            
            audio_buffer = BytesIO()
            combined.export(audio_buffer, format="wav")
            audio_buffer.seek(0)

           
            import speech_recognition as sr

            r = sr.Recognizer()
            with sr.AudioFile(audio_buffer) as source:
                audio_data = r.record(source)
                try:
                    user_input = r.recognize_google(audio_data)
                    st.write(f"ë‹¹ì‹ ì´ ë§í•œ ë‚´ìš©: {user_input}")

               
                    current_line = initial_script[st.session_state.current_line]
                    accuracy = evaluate_speech_accuracy(current_line, user_input)
                    st.write(f"ìŒì„± ì¸ì‹ ì •í™•ë„: {accuracy:.2f}%")

                    if accuracy >= 90:
                        st.success("ë°œìŒì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤!")
                    elif accuracy >= 70:
                        st.info("ì¢‹ì€ ë°œìŒì…ë‹ˆë‹¤. ê³„ì† ì—°ìŠµí•˜ì„¸ìš”!")
                    else:
                        st.warning("ë°œìŒ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”!")
                except sr.UnknownValueError:
                    st.error("ìŒì„±ì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except sr.RequestError:
                    st.error("ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ì— ìš”ì²­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.write("ë…¹ìŒì„ ë§ˆì¹œ í›„ 'ì˜¤ë””ì˜¤ ì²˜ë¦¬í•˜ê¸°'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

if st.button("ğŸš€ ì œì¶œ") and 'user_input' in locals():
    with st.spinner("AI íŠœí„°ê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
        ai_response = generate_response(user_input)
    st.success("AI íŠœí„°: " + ai_response)
    if st.button("ğŸ”Š AI ì‘ë‹µ ë“£ê¸°"):
        text_to_speech(ai_response)

st.header("ğŸ“œ ëŒ€í™” ê¸°ë¡")
for message in st.session_state.conversation_history:
    if message['role'] == 'user':
        st.markdown(f"**ì‚¬ìš©ì:** {message['content']}")
    else:
        st.markdown(f"**AI íŠœí„°:** {message['content']}")